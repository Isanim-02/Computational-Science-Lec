{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "095805f1",
   "metadata": {},
   "source": [
    "# Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181c55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def consolidate_csv_files(datasets_folder, output_path, columns_to_drop= ['latitude', 'longitude']):\n",
    "    # Load CSVs that are not called 'consolidated.csv'\n",
    "    csv_files = [\n",
    "        os.path.join(datasets_folder, f)\n",
    "        for f in os.listdir(datasets_folder)\n",
    "        if f.endswith(\".csv\") and f != \"consolidated.csv\"\n",
    "    ]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found to merge.\")\n",
    "    else:\n",
    "        # Load all datasets into a list\n",
    "        dataframes = []\n",
    "        for f in csv_files:\n",
    "            df = pd.read_csv(f)\n",
    "            # Drop unwanted columns if they exist\n",
    "            df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "            dataframes.append(df)\n",
    "\n",
    "        # Check that all columns match\n",
    "        first_cols = dataframes[0].columns.tolist()\n",
    "        all_match = all(df.columns.tolist() == first_cols for df in dataframes)\n",
    "\n",
    "        if all_match:\n",
    "            print(\"Columns match, merging all files...\")\n",
    "            df_all = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "            output_path = os.path.join(datasets_folder, \"consolidated.csv\")\n",
    "            df_all.to_csv(output_path, index=False)\n",
    "            print(f\"Merged CSV saved as '{output_path}'\")\n",
    "        else:\n",
    "            print(\"WARNING: Not all CSV files have matching columns!\")\n",
    "            for i, df in enumerate(dataframes):\n",
    "                print(f\"{csv_files[i]} columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f2bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge daily datasets\n",
    "consolidate_csv_files(datasets_folder=\"../datasets/daily\", output_path=\"../datasets/daily/consolidated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12a032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns match, merging all files...\n",
      "Merged CSV saved as '../datasets/hourly\\consolidated.csv'\n"
     ]
    }
   ],
   "source": [
    "# Merge hourly datasets\n",
    "consolidate_csv_files(datasets_folder=\"../datasets/hourly\", output_path=\"../datasets/daily/consolidated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899011b",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4978a9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "Cities: 141\n",
      "Daily records: 768797\n",
      "Hourly records: 18451128\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING DATA\n",
      "======================================================================\n",
      "Aggregating hourly to daily...\n",
      "Aggregating daily to monthly...\n",
      "\n",
      "Adding cyclical time encoding (sin/cos for months)...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_data():\n",
    "    daily_data_path='../datasets/daily/consolidated.csv'\n",
    "    hourly_data_path='../datasets/hourly/consolidated.csv'\n",
    "    cities_path='../datasets/cities.csv'\n",
    "    df_monthly = None\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Load cities\n",
    "    df_cities = pd.read_csv(cities_path)\n",
    "    df_cities.rename(columns={'city_name': 'city'}, inplace=True)\n",
    "    print(f\"Cities: {len(df_cities)}\")\n",
    "\n",
    "    # Load daily data\n",
    "    df_daily = pd.read_csv(daily_data_path)\n",
    "    df_daily.rename(columns={'city_name': 'city', 'datetime': 'date'}, inplace=True)\n",
    "    print(f\"Daily records: {len(df_daily)}\")\n",
    "\n",
    "    # Load hourly data\n",
    "    df_hourly = pd.read_csv(hourly_data_path)\n",
    "    df_hourly.rename(columns={\n",
    "        'city_name': 'city', \n",
    "        'datetime': 'date',\n",
    "        'relative_humidity_2m': 'humidity_2m'\n",
    "    }, inplace=True)\n",
    "    print(f\"Hourly records: {len(df_hourly)}\")\n",
    "\n",
    "    ### PREPROCESS DATA \n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPROCESSING DATA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Convert dates\n",
    "    df_daily['date'] = pd.to_datetime(df_daily['date'])\n",
    "    df_hourly['date'] = pd.to_datetime(df_hourly['date'])\n",
    "\n",
    "    # Aggregate hourly to daily\n",
    "    print(\"Aggregating hourly to daily...\")\n",
    "    hourly_daily = df_hourly.groupby(['city', 'date']).agg({\n",
    "        'humidity_2m': 'mean',\n",
    "        'surface_pressure': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge with daily data\n",
    "    df_merged = df_daily.merge(hourly_daily, on=['city', 'date'], how='left')\n",
    "\n",
    "    # Aggregate to monthly\n",
    "    print(\"Aggregating daily to monthly...\")\n",
    "    df_merged['year'] = df_merged['date'].dt.year\n",
    "    df_merged['month'] = df_merged['date'].dt.month\n",
    "\n",
    "    df_monthly = df_merged.groupby(['city', 'year', 'month']).agg({\n",
    "        'temperature_2m_mean': 'mean',\n",
    "        'humidity_2m': 'mean',\n",
    "        'surface_pressure': 'mean',\n",
    "        'precipitation_sum': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    df_monthly.rename(columns={\n",
    "        'temperature_2m_mean': 'temperature',\n",
    "        'humidity_2m': 'humidity',\n",
    "        'surface_pressure': 'air_pressure',\n",
    "        'precipitation_sum': 'monthly_rainfall'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Merge with coordinates\n",
    "    df_monthly = df_monthly.merge(\n",
    "        df_cities[['city', 'latitude', 'longitude']], \n",
    "        on='city', \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Add cyclical time encoding for months\n",
    "    print(\"\\nAdding cyclical time encoding (sin/cos for months)...\")\n",
    "    df_monthly['month_sin'] = np.sin(2 * np.pi * df_monthly['month'] / 12)\n",
    "    df_monthly['month_cos'] = np.cos(2 * np.pi * df_monthly['month'] / 12)\n",
    "\n",
    "    # Sort to ensure proper temporal order\n",
    "    df_monthly = df_monthly.sort_values(\n",
    "        by=['city', 'year', 'month']\n",
    "    )\n",
    "\n",
    "    # Add lagged rainfall per city\n",
    "    grouped = df_monthly.groupby('city')['monthly_rainfall']\n",
    "    df_monthly['monthly_rainfall_lag_1'] = grouped.shift(1)\n",
    "\n",
    "    return df_monthly\n",
    "\n",
    "\n",
    "month_to_season = {\n",
    "    1: 'DJF', 2: 'DJF', 3: 'JFM',\n",
    "    4: 'FMA', 5: 'FMA', 6: 'AMJ',\n",
    "    7: 'MJJ', 8: 'JJA', 9: 'JAS',\n",
    "    10: 'ASO', 11: 'SON', 12: 'NDJ'\n",
    "}\n",
    "\n",
    "# Add ENSO indices\n",
    "def add_enso_indices(df, oni_data_path='../datasets/oni_indices.csv'):\n",
    "    \"\"\"Add ENSO (El NiÃ±o Southern Oscillation) indices\"\"\"\n",
    "    oni_data = pd.read_csv(oni_data_path, index_col='year')\n",
    "\n",
    "    def get_oni(row):\n",
    "        if row['year'] in oni_data.index:\n",
    "            season_col = month_to_season.get(row['month'])\n",
    "            return oni_data.at[row['year'], season_col]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    df['oni_index'] = df.apply(get_oni, axis=1)\n",
    "    df['el_nino'] = (df['oni_index'] > 0.5).astype(int)\n",
    "    df['la_nina'] = (df['oni_index'] < -0.5).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_monthly = preprocess_data()\n",
    "add_enso_indices(df_monthly)\n",
    "\n",
    "df_monthly.to_csv('../datasets/monthly.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
